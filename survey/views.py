import json
import os
import sqlite3
from collections import defaultdict
import re

import networkx as nx
import pandas as pd
from django.contrib.auth import login, logout
from django.contrib.auth.decorators import login_required

from news_analyzer.daily_analysis import preprocess_text
from news_analyzer.daily_analysis.wordcloud import extract_keywords
from news_analyzer.database import fetch_news_data
from .models import Question, InvestmentResult
from django.shortcuts import render, redirect
from django.contrib.auth.forms import UserCreationForm
from django.http import HttpResponse
from datetime import datetime, timedelta


# ë©”ì¸ í˜ì´ì§€
def index_view(request):
    return render(request, 'survey/index.html')



# ë¡œê·¸ì¸ & ë¡œê·¸ì•„ì›ƒ
def signup_view(request):
    if request.method == 'POST':
        form = UserCreationForm(request.POST)
        if form.is_valid():
            user = form.save()
            login(request, user) # íšŒì›ê°€ì… í›„ ìë™ ë¡œê·¸ì¸ ì²˜ë¦¬
            return redirect('News_home')  # í™ˆ í˜ì´ì§€ë¡œ ë¦¬ë””ë ‰ì…˜
    else:
        form = UserCreationForm()
    return render(request, 'survey/signup.html', {'form': form})

def logout_view(request):
    logout(request)
    return redirect('News_home')




# ë§ˆì´í˜ì´ì§€
def mypage_view(request):
    # ì‚¬ìš©ìì˜ ê°€ì¥ ìµœì‹  ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    latest_result = InvestmentResult.objects.filter(user_id=request.user.id).order_by('-id').first()

    # ê²°ê³¼ ìœ í˜• ê²°ì •
    if latest_result:
        user_mbti_type = latest_result.result_type
    else:
        user_mbti_type = "ê²°ê³¼ ì—†ìŒ"

    # ê³µí†µ ë°ì´í„°ì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    results = get_mbti_data()
    user_result = results.get(user_mbti_type, {
        "mbti_type": "ê¸ˆìœµ ì„±í–¥ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•´ë³´ì„¸ìš”",
        "mbti_description": "ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤",
        "detailed_description01": "",
        "image_path": "survey/images/person.jpg",
    })

    # ì¶”ê°€ ë°ì´í„°ë¥¼ ë§ˆì´í˜ì´ì§€ì— ì „ë‹¬
    stock_list = [
        {'name': 'ì‚¼ì„±ì „ì', 'price': '71,500ì›', 'description': 'ì „ì ì œí’ˆ ëŒ€ê¸°ì—…'},
        {'name': 'ì¹´ì¹´ì˜¤', 'price': '123,000ì›', 'description': 'IT í”Œë«í¼ ê¸°ì—…'},
    ]

    context = {
        'mbti_type': user_result['mbti_type'],
        'mbti_description': user_result.get('mbti_description', 'ê¸°ë³¸ ì„¤ëª…ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.'),
        'detailed_description01': user_result['detailed_description01'],
        'image_path': user_result.get('image_path', 'survey/images/person.jpg'),
        'stock_list': stock_list,
    }

    return render(request, 'survey/mypage.html', context)





# ì£¼ì‹ ì¶”ì²œ
@login_required
def stock_recommendations(request):
    return render(request, 'survey/stock_recommendations.html')




# News
def News_home(request):
    news_data = fetch_news_data()
    recent_news = []

    # ìµœì‹  ë‰´ìŠ¤ (5ê°œ)
    for title, content, url in news_data[:5]:
        recent_news.append({
            'title': title,
            'url': url,
        })

    # ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ì •ì˜
    categories = {
        'ê¸ˆìœµ': r'ê¸ˆìœµ|ì€í–‰|ì €ì¶•|ëŒ€ì¶œ|ì´ì',
        'ì¦ê¶Œ': r'ì¦ê¶Œ|ì£¼ì‹|ì½”ìŠ¤í”¼|ì½”ìŠ¤ë‹¥|ì±„ê¶Œ',
        'ë¶€ë™ì‚°': r'ë¶€ë™ì‚°|ì•„íŒŒíŠ¸|ì£¼íƒ|ì „ì„¸|ì›”ì„¸',
        'ê²½ì œ ì¼ë°˜': r'ê²½ì œ|ì‚°ì—…|ë¬´ì—­|ìˆ˜ì¶œ|ê¸°ì—…'
    }

    categorized_news = defaultdict(list)

    # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
    for title, content, url in news_data:
        categorized = False
        for category, pattern in categories.items():
            if re.search(pattern, title) or re.search(pattern, content):
                if len(categorized_news[category]) < 4:
                    categorized_news[category].append({
                        'title': title,
                        'url': url  # URL ì¶”ê°€
                    })
                    categorized = True
                    break

        # ë¶„ë¥˜ë˜ì§€ ì•Šì€ ë‰´ìŠ¤ëŠ” 'ê²½ì œ ì¼ë°˜'ì— ì¶”ê°€
        if not categorized and len(categorized_news['ê²½ì œ ì¼ë°˜']) < 4:
            categorized_news['ê²½ì œ ì¼ë°˜'].append({
                'title': title,
                'url': url  # URL ì¶”ê°€
            })

    # í•«í‚¤ì›Œë“œ ì¶”ì¶œ
    word_count = defaultdict(int)
    for title, content, url in news_data:
        words = re.findall(r'\w+', title)
        for word in words:
            if len(word) >= 2:  # 2ê¸€ì ì´ìƒì˜ ë‹¨ì–´ë§Œ ì¹´ìš´íŠ¸
                word_count[word] += 1

    # ê°€ì¥ ë§ì´ ë“±ì¥í•œ 5ê°œ ë‹¨ì–´ë¥¼ í•«í‚¤ì›Œë“œë¡œ ì„ ì •
    keywords = sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:5]
    hot_keywords = [keyword for keyword, _ in keywords]

    # ì»¨í…ìŠ¤íŠ¸ ìƒì„± ë° ë Œë”ë§
    context = {
        'recent_news': recent_news,
        'categorized_news': dict(categorized_news),
        'hot_keywords': hot_keywords,
    }
    return render(request, 'survey/News_home.html', context)


@login_required
def hot_topic(request):
    try:
        conn = sqlite3.connect("news_analyzer/inbest.db")
        cursor = conn.cursor()

        cursor.execute("""
           SELECT title, content 
           FROM news 
           ORDER BY date DESC 
           LIMIT 50
       """)

        news_data = cursor.fetchall()

        # ê°ì„±ì–´ ì‚¬ì „ ë¡œë“œ
        base_path = os.path.join(os.path.dirname(__file__), '..', 'news_analyzer', 'csv')
        senti_lex = pd.read_csv(os.path.join(base_path, 'SentiWord_Dict.csv'))
        positive_words = set(senti_lex[senti_lex['polarity'] > 0]['word'])
        negative_words = set(senti_lex[senti_lex['polarity'] < 0]['word'])

        positive_news = []
        negative_news = []
        main_positive_news = None
        main_negative_news = None

        # ë‰´ìŠ¤ ë¶„ë¥˜
        for title, content in news_data:
            words = set(re.findall(r'[ê°€-í£]+', title))
            pos_score = len(words & positive_words)
            neg_score = len(words & negative_words)

            news_item = {
                'title': title,
                'content': content
            }

            if pos_score > neg_score:
                if not main_positive_news:  # ì²« ë²ˆì§¸ ê¸ì • ë‰´ìŠ¤ë¥¼ ë©”ì¸ ë‰´ìŠ¤ë¡œ
                    main_positive_news = news_item
                elif len(positive_news) < 4:  # ë‚˜ë¨¸ì§€ëŠ” ëª©ë¡ì—
                    positive_news.append(news_item)
            elif neg_score > pos_score:
                if not main_negative_news:  # ì²« ë²ˆì§¸ ë¶€ì • ë‰´ìŠ¤ë¥¼ ë©”ì¸ ë‰´ìŠ¤ë¡œ
                    main_negative_news = news_item
                elif len(negative_news) < 4:  # ë‚˜ë¨¸ì§€ëŠ” ëª©ë¡ì—
                    negative_news.append(news_item)

        # ë¶€ì¡±í•œ ê¸°ì‚¬ ì±„ìš°ê¸°
        if not main_positive_news:
            main_positive_news = {
                'title': 'ì˜¤ëŠ˜ì€ ì£¼ìš” í˜¸ì¬ê°€ ì—†ìŠµë‹ˆë‹¤',
                'content': 'í˜„ì¬ ì‹œì¥ì— ì£¼ìš”í•œ í˜¸ì¬ì„± ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }
        if not main_negative_news:
            main_negative_news = {
                'title': 'ì˜¤ëŠ˜ì€ ì£¼ìš” ì•…ì¬ê°€ ì—†ìŠµë‹ˆë‹¤',
                'content': 'í˜„ì¬ ì‹œì¥ì— ì£¼ìš”í•œ ì•…ì¬ì„± ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }

        while len(positive_news) < 4:
            positive_news.append({
                'title': '',
                'content': 'ì¶”ê°€ í˜¸ì¬ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.'
            })
        while len(negative_news) < 4:
            negative_news.append({
                'title': '',
                'content': 'ì¶”ê°€ ì•…ì¬ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.'
            })

        context = {
            'main_positive_news': main_positive_news,
            'main_negative_news': main_negative_news,
            'positive_news': positive_news,
            'negative_news': negative_news,
        }

    except Exception as e:
        print(f"Error: {str(e)}")
        default_item = {
            'title': 'ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
            'content': 'ë‚´ìš©ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
        }
        context = {
            'main_positive_news': default_item,
            'main_negative_news': default_item,
            'positive_news': [default_item] * 4,
            'negative_news': [default_item] * 4
        }

    finally:
        if 'conn' in locals():
            conn.close()

    return render(request, 'survey/hot_topic.html', context)


@login_required
def daily_analysis(request):
    def convert_kr_date(date_str):
        try:
            # ë§ˆì¹¨í‘œ ì œê±° ë° ë¶„ë¦¬
            parts = date_str.replace('.', '').split()

            # ë‚ ì§œ ë¶€ë¶„ ì²˜ë¦¬
            date_nums = parts[0].split()
            year = int(date_nums[0][:4])
            month = int(date_nums[0][4:6])
            day = int(date_nums[0][6:8])

            # ì‹œê°„ ë¶€ë¶„ ì²˜ë¦¬
            time_parts = parts[2].split(':')
            hour = int(time_parts[0])
            minute = int(time_parts[1])

            # ì˜¤ì „/ì˜¤í›„ ì²˜ë¦¬
            if parts[1] == 'ì˜¤í›„' and hour < 12:
                hour += 12
            elif parts[1] == 'ì˜¤ì „' and hour == 12:
                hour = 0

            return datetime(year, month, day, hour, minute).date()
        except Exception as e:
            print(f"Date conversion error for {date_str}: {str(e)}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ í˜„ì¬ ë‚ ì§œ ë°˜í™˜
            return datetime.now().date()

    try:
        # inbest.db ì—°ê²°
        db_path = os.path.join("news_analyzer", "inbest.db")
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()

        # 7ì¼ê°„ì˜ ë‰´ìŠ¤ ë°ì´í„° ì¡°íšŒ
        cursor.execute("""
            SELECT title, content, author, date 
            FROM news 
            WHERE date >= datetime('now', '-7 day')
            ORDER BY date DESC;
        """)

        news_data = cursor.fetchall()
        print(f"Total news data fetched: {len(news_data)}")

        if news_data:
            # ë°ì´í„°í”„ë ˆì„ ìƒì„±
            df = pd.DataFrame(news_data, columns=['title', 'content', 'author', 'date'])

            # ë‚ ì§œ ë³€í™˜
            df['date'] = df['date'].apply(convert_kr_date)

            print(f"DataFrame shape: {df.shape}")

            # ì›Œë“œí´ë¼ìš°ë“œ ë°ì´í„° ìƒì„±
            cloud_data = extract_keywords(df[['title', 'content', 'author']].values.tolist())
            print(f"Word cloud data length: {len(cloud_data)}")

            # ì¹´í…Œê³ ë¦¬ ì •ì˜
            categories = {
                'ê²½ì œ': ['ê¸ˆë¦¬', 'ì£¼ì‹', 'GDP', 'ë¬¼ê°€', 'ê²½ê¸°', 'ì›í™”'],
                'íˆ¬ì': ['ETF', 'ì±„ê¶Œ', 'í€ë“œ', 'ë°°ë‹¹', 'íˆ¬ì', 'ìˆ˜ìµ'],
                'ê¸°ì—…': ['ì‹¤ì ', 'ê³µì‹œ', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ê¸°ì—…', 'ì„±ì¥'],
                'ì •ì±…': ['ì •ë¶€', 'ê·œì œ', 'ì •ì±…', 'ë²•ì•ˆ', 'ì œë„', 'ë‹¹êµ­']
            }

            # ë„¤íŠ¸ì›Œí¬ ë°ì´í„° ìƒì„±
            network_data = {
                'nodes': [{'id': 'center', 'label': 'í‚¤ì›Œë“œ', 'category': 'center'}],
                'edges': []
            }

            # ì›Œë“œí´ë¼ìš°ë“œ ë°ì´í„°ì—ì„œ ë‚˜ì˜¨ ë‹¨ì–´ë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë§¤í•‘
            word_set = set(item['text'] for item in cloud_data)

            # ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ì¶”ê°€
            for category, keywords in categories.items():
                matching_keywords = [word for word in keywords if word in word_set]
                if matching_keywords:
                    network_data['nodes'].append({
                        'id': category,
                        'label': category,
                        'category': 'main'
                    })
                    network_data['edges'].append({
                        'source': 'center',
                        'target': category,
                        'weight': 3
                    })

                    for word in matching_keywords:
                        network_data['nodes'].append({
                            'id': word,
                            'label': word,
                            'category': 'keyword'
                        })
                        network_data['edges'].append({
                            'source': category,
                            'target': word,
                            'weight': 2
                        })

            print(f"Network data nodes: {len(network_data['nodes'])}, edges: {len(network_data['edges'])}")

            # ì–¸ë¡ ì‚¬ë³„ ì¼ê°„ ë³´ë„ í˜„í™©
            press_daily = df.groupby(['date', 'author']).size().unstack(fill_value=0)

            # ìƒìœ„ 7ê°œ ì–¸ë¡ ì‚¬ ì„ íƒ
            top_authors = df['author'].value_counts().nlargest(7).index
            press_daily = press_daily[top_authors]

            press_data = {
                'dates': [d.strftime('%Y-%m-%d') for d in press_daily.index],
                'authors': list(press_daily.columns),
                'values': press_daily.values.tolist()
            }

            print(f"Press data dates: {len(press_data['dates'])}, authors: {len(press_data['authors'])}")

            context = {
                'wordcloud_data': json.dumps(cloud_data, ensure_ascii=False),
                'network_data': json.dumps(network_data, ensure_ascii=False),
                'press_data': json.dumps(press_data, ensure_ascii=False),
                'analysis_date': datetime.now().strftime('%Y.%m.%d %H:%M')
            }
        else:
            context = {
                'wordcloud_data': json.dumps([{"text": "ë°ì´í„° ì—†ìŒ", "size": 50, "sentiment": "neutral"}]),
                'network_data': json.dumps({'nodes': [], 'edges': []}),
                'press_data': json.dumps({'dates': [], 'authors': [], 'values': []}),
                'analysis_date': datetime.now().strftime('%Y.%m.%d %H:%M')
            }

    except Exception as e:
        print(f"Error occurred: {str(e)}")
        print(f"Error type: {type(e)}")
        import traceback
        print(f"Traceback: {traceback.format_exc()}")
        context = {
            'wordcloud_data': json.dumps([{"text": "ì˜¤ë¥˜ ë°œìƒ", "size": 50, "sentiment": "neutral"}]),
            'network_data': json.dumps({'nodes': [], 'edges': []}),
            'press_data': json.dumps({'dates': [], 'authors': [], 'values': []}),
            'analysis_date': datetime.now().strftime('%Y.%m.%d %H:%M')
        }

    finally:
        if 'conn' in locals():
            conn.close()

    return render(request, 'survey/daily_analysis.html', context)


# ê¸ˆìœµì„±í–¥í…ŒìŠ¤íŠ¸
def get_mbti_data():
    return {
        "ì‚¬ì": {
            "mbti_type": "ì‚¬ì",
            "mbti_description": "í•˜ì´ ë¦¬ìŠ¤í¬ì—ë„ ì£¼ì‹ ì‚¬ì!",
            "detailed_description01": '''
               ë‘ë ¤ì›€ì€ ìˆ˜ìµì„ ëŠ¦ì¶œ ë¿ğŸ¦
               ''',

            "detailed_description02": '''
               íˆ¬ìì—ì„œ ë†’ì€ ë¦¬ìŠ¤í¬ë¥¼ ê°ìˆ˜í•˜ë©°, 
               í° ìˆ˜ìµì„ ê¸°ëŒ€í•©ë‹ˆë‹¤. 
               ë³€ë™ì„±ì´ í° ì‹œì¥ì—ì„œë„ 
               ê³¼ê°í•œ ê²°ì •ì„ ë‚´ë¦¬ëŠ” í¸ì…ë‹ˆë‹¤. 
               ì£¼ì‹, ì•”í˜¸í™”í, ì‹ ê¸°ìˆ  ê´€ë ¨ ìì‚°ì— 
               ì ê·¹ì ìœ¼ë¡œ íˆ¬ìí•©ë‹ˆë‹¤.   
               ë‹¨ê¸°ì  ìˆ˜ìµ ì‹¤í˜„ì— ì´ˆì ì„ ë§ì¶”ë©°, 
               ì£¼ì‹ì‹œì¥ì˜ ê¸‰ê²©í•œ ë³€ë™ì—ë„ 
               ë¶ˆì•ˆê°ì„ ëŠë¼ì§€ ì•Šê³  
               ì ê·¹ì ìœ¼ë¡œ ëŒ€ì‘í•©ë‹ˆë‹¤.  
               ''',

            "detailed_description03": '''
                ì£¼ì‹ ë° ì•”í˜¸í™”íì™€ ê°™ì€ ê³ ìœ„í—˜ ìì‚°ì— íˆ¬ìí•˜ë˜, 
                ì ì •í•œ ë¹„ìœ¨ë¡œ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë¶„ì‚°í•˜ì„¸ìš”.   
                ê¸°ìˆ ì  ë¶„ì„ê³¼ ë§¤ìˆ˜/ë§¤ë„ ì‹œì ì„ 
                ì •í™•íˆ íŒë‹¨í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.   
                ë³€ë™ì„±ì´ ë†’ì€ ì‹œì¥ì—ì„œëŠ” 
                ì†ì ˆë§¤ ê¸°ì¤€ì„ ëª…í™•íˆ ì„¤ì •í•˜ì„¸ìš”. 
               ''',

            "image_path": "survey/images/lion.png",
        },
        "ë…ìˆ˜ë¦¬": {
            "mbti_type": "ë…ìˆ˜ë¦¬",
            "mbti_description": "ê¸°íšŒë¥¼ í¬ì°©í•˜ëŠ” ìˆ˜ë¦¬!",
            "detailed_description01": '''
               íƒ€ì´ë°ì€ ë‚´ê°€ ì •í•´ğŸ¦…
               ''',

            "detailed_description02": '''
                ì•ˆì •ì„±ê³¼ ì„±ì¥ ì‚¬ì´ì—ì„œ ê· í˜•ì„ ë§ì¶”ë©°, 
                ìƒí™©ì— ë”°ë¼ ê¸°íšŒë¥¼ í¬ì°©í•˜ì—¬ íˆ¬ìí•©ë‹ˆë‹¤. 
                ë¦¬ìŠ¤í¬ë¥¼ íšŒí”¼í•˜ì§€ ì•Šìœ¼ë©´ì„œ
                ë¬´ë¦¬í•˜ì§€ ì•ŠëŠ” ì„ ì—ì„œ ë‹¤ì–‘í•œ ìì‚°ì— íˆ¬ìí•©ë‹ˆë‹¤.
                ì‹œì¥ì„ ë©´ë°€íˆ ë¶„ì„í•˜ê³  
                ì‹ ì¤‘í•˜ê²Œ íˆ¬ìí•˜ë©°, 
                ê³ ìœ„í—˜ ê³ ìˆ˜ìµ ìƒí’ˆì—ë„ 
                ì ë‹¹íˆ ë„ì „í•©ë‹ˆë‹¤.
               ''',

            "detailed_description03": '''        
                    ì‹œì¥ì˜ ê¸°íšŒë¥¼ íƒìƒ‰í•  ë•Œ 
                    ì§€ë‚˜ì¹˜ê²Œ ì‹ ì¤‘í•´ì§€ì§€ ì•Šë„ë¡ 
                    ì ì ˆí•œ ê²°ë‹¨ë ¥ì„ ê°€ì§€ì„¸ìš”.
                    ë¦¬ìŠ¤í¬ê°€ í° ìì‚°ê³¼ ì•ˆì „ ìì‚°ì„ 
                    ì ì ˆíˆ ë°°ë¶„í•˜ì—¬ ì•ˆì •ì„±ì„ ìœ ì§€í•˜ì„¸ìš”.
                    ìƒˆë¡œìš´ íˆ¬ì ê¸°íšŒì— ì—´ë¦° íƒœë„ë¥¼ ê°€ì§€ë˜, 
                    ì¶©ë¶„í•œ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  
                    ë¶„ì„í•œ í›„ ê²°ì •í•˜ì„¸ìš”.
               ''',

            "image_path": "survey/images/eagle.png",
        },
        "ê±°ë¶ì´": {
            "mbti_type": "ê±°ë¶ì´",
            "mbti_description": "ëŠë¦¬ì§€ë§Œ ê¾¸ì¤€í•œ ë¶ì´!",
            "detailed_description01": '''
                ëê¹Œì§€ ê°€ë©´ ë‚´ê°€ ë‹¤ ì´ê²¨!ğŸ¢
               ''',
            "detailed_description02": '''
                    íˆ¬ìì—ì„œ ì•ˆì •ì„±ê³¼ ì¥ê¸°ì  ì„±ì¥ì„ ì¤‘ì‹œí•©ë‹ˆë‹¤. 
                    ìœ„í—˜ì„ ìµœì†Œí™”í•˜ë©´ì„œ ê¾¸ì¤€íˆ ìì‚°ì„ 
                    ëŠ˜ë ¤ê°€ëŠ” ë°©ì‹ì„ ì„ í˜¸í•©ë‹ˆë‹¤.
                    ë³€ë™ì„±ì´ ë‚®ì€ ìì‚°ì— íˆ¬ìí•˜ë©°, 
                    í™•ì‹¤í•œ ê¸°íšŒë¥¼ ê¸°ë‹¤ë¦¬ëŠ” ì „ëµì„ í™œìš©í•©ë‹ˆë‹¤!
                   ''',

            "detailed_description03": '''
                    ì¥ê¸°ì ì¸ ì•ˆì •ì  ìˆ˜ìµì„ ìœ„í•´ êµ­ì±„, ê³ ì • ìˆ˜ìµ ìƒí’ˆ, 
                    ìš°ëŸ‰ì£¼ ë° ETFë¥¼ í™œìš©í•œ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”.
                    ë‹¨ê¸°ì ì¸ ì‹œì¥ ë³€ë™ì— ë¯¼ê°í•˜ì§€ ì•Šê³ , 
                    ê¾¸ì¤€íˆ ëª©í‘œë¥¼ ìœ ì§€í•˜ì„¸ìš”.
                    ì •ê¸°ì ìœ¼ë¡œ íˆ¬ì ì„±ê³¼ë¥¼ ê²€í† í•˜ë©°, 
                    í•„ìš”ì— ë”°ë¼ ì¡°ì •í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.
                   ''',
            "image_path": "survey/images/turtle.png",
        },
        "ê³ ìŠ´ë„ì¹˜": {
            "mbti_type": "ê³ ìŠ´ë„ì¹˜",
            "mbti_description": "ì•ˆì „ ì œì¼ ë„ì¹˜!",
            "detailed_description01": '''
               ìƒì§€ ì•ŠëŠ”ê²Œ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤êµ¬ğŸ¦”
               ''',
            "detailed_description02": '''
                    ìì‚° ë³´í˜¸ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ìƒê°í•˜ë©°, 
                    ì•ˆì •ì ì´ê³  ë³€ë™ì„±ì´ 
                    ì ì€ íˆ¬ì ìƒí’ˆì„ ì„ í˜¸í•©ë‹ˆë‹¤. 
                    ë¦¬ìŠ¤í¬ë¥¼ ê·¹ë„ë¡œ êº¼ë¦¬ë©°,
                    ìˆ˜ìµë³´ë‹¤ëŠ” ìë³¸ ë³´ì¡´ì´ ëª©í‘œì…ë‹ˆë‹¤.
                    êµ­ì±„, ê³ ì • ìˆ˜ìµ ìƒí’ˆ, ì˜ˆê¸ˆ ë“±ì— ì£¼ë¡œ íˆ¬ìí•˜ë©°,
                    ì‹œì¥ ë³€ë™ì„±ì— ë§¤ìš° ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•©ë‹ˆë‹¤. 
                    ''',

            "detailed_description03": '''
                    êµ­ì±„ë‚˜ ê³ ì • ìˆ˜ìµ ìƒí’ˆ ìœ„ì£¼ë¡œ 
                    í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ êµ¬ì„±í•˜ë˜, 
                    ë¬¼ê°€ ìƒìŠ¹ë¥ ì— ëŒ€ë¹„í•´ 
                    ì¼ë¶€ ì„±ì¥í˜• ìì‚°ë„ ì†ŒëŸ‰ í¬í•¨í•˜ì„¸ìš”.
                    ê¸‰ê²©í•œ ì‹œì¥ ë³€ë™ì—ì„œ íŒ¨ë‹‰ ë§¤ë„ë¥¼ í”¼í•˜ì„¸ìš”.
                    ì „ë¬¸ê°€ì˜ ì¡°ì–¸ì´ë‚˜ í€ë“œ ë§¤ë‹ˆì €ë¥¼ í™œìš©í•œ
                    ë³´ìˆ˜ì  íˆ¬ì ë°©ë²•ì„ ê³ ë ¤í•˜ì„¸ìš”.
                   ''',
            "image_path": "survey/images/hedgehog.png",
        },
    }

@login_required
def mbti_test(request):
    if request.method == "GET":
        # í…ŒìŠ¤íŠ¸ í˜ì´ì§€ í‘œì‹œ
        return render(request, 'survey/mbti_test.html')
    elif request.method == "POST":
        # ì œì¶œëœ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        total_score = int(request.POST.get('total_score', 0))

        # ì ìˆ˜ì— ë”°ë¥¸ ê²°ê³¼ ê³„ì‚°
        if total_score >= 45:
            result_type = "ì‚¬ì"
        elif total_score >= 30:
            result_type = "ë…ìˆ˜ë¦¬"
        elif total_score >= 15:
            result_type = "ê±°ë¶ì´"
        else:
            result_type = "ê³ ìŠ´ë„ì¹˜"

        # ê²°ê³¼ ì €ì¥
        InvestmentResult.objects.create(
            user_id=request.user.id,
            total_score=total_score,
            result_type=result_type
        )

        # ì„¸ì…˜ì— ê²°ê³¼ ì €ì¥
        request.session['mbti_result'] = {
            'type': result_type,
            'description': f"{result_type} ìœ í˜• ì„¤ëª…"
        }

        # ê²°ê³¼ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
        return redirect('mbti_result')

@login_required
def mbti_result(request):
    # ì„¸ì…˜ì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    mbti_result = request.session.get('mbti_result')

    if not mbti_result:
        return HttpResponse("í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

    # ê³µí†µ ë°ì´í„°ì—ì„œ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
    results = get_mbti_data()
    result_data = results.get(mbti_result['type'], {
        "mbti_type": "ì•Œ ìˆ˜ ì—†ìŒ",
        "mbti_description" : "ì„¤ëª…ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
        "detailed_description01": "ì„¤ëª…ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
        "detailed_description02": "ì„¤ëª…ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
        "detailed_description03": "ì„¤ëª…ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
        "image_path": "",
    })


    # ë°ì´í„° ì²˜ë¦¬
    mbti_description = result_data.get('mbti_description', 'ì„¤ëª…ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
    detailed_description01 = result_data.get('detailed_description01', 'ì„¤ëª…ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
    detailed_description02 = result_data.get('detailed_description02', 'ì„¤ëª…ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
    detailed_description03 = result_data.get('detailed_description03', 'ì„¤ëª…ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')

    return render(request, 'survey/mbti_result.html', {
        'result_type': result_data['mbti_type'],
        'mbti_description' : mbti_description,
        'detailed_description01': detailed_description01,
        'detailed_description02': detailed_description02,
        'detailed_description03': detailed_description03,
        'image_path': result_data.get('image_path', ''),
    })

